lesson:
  id: "sklearn_12"
  level: "intermediate"
  topic: "sklearn"
  subtopic: "Feature Selection"
  order: 12

  metadata:
    duration: "35 min"
    difficulty: "medium"
    prerequisites: ["sklearn_05", "sklearn_06", "sklearn_10"]
    learning_objectives:
      - "Understand why feature selection matters"
      - "Use statistical methods for feature selection"
      - "Apply model-based feature selection"
      - "Implement recursive feature elimination"

  content:
    introduction: |
      # Feature Selection

      Less is more! Learn to identify and keep only the most important features,
      improving model performance and reducing complexity.

      **What you'll learn:**
      - Why select features
      - Statistical methods
      - Model-based selection
      - Recursive feature elimination

    concept: |
      ## Choosing the Right Features

      **1. Why Feature Selection?**

      **Benefits:**
      - ✅ Faster training (fewer features)
      - ✅ Reduced overfitting
      - ✅ Improved model interpretability
      - ✅ Better generalization
      - ✅ Remove irrelevant/redundant features

      **When you have too many features:**
      - Training is slow
      - Model is complex
      - Risk of overfitting
      - Hard to interpret

      **2. Types of Feature Selection**

      **Filter Methods** (statistical tests):
      - Independent of model
      - Fast and scalable
      - Examples: correlation, chi-squared, ANOVA
      - Use before modeling

      **Wrapper Methods** (use model performance):
      - Model-dependent
      - More accurate but slower
      - Examples: RFE, forward/backward selection
      - Computationally expensive

      **Embedded Methods** (built into model):
      - Feature selection during training
      - Examples: Lasso, Tree feature importance
      - Good balance of speed and accuracy

      **3. Statistical Methods - SelectKBest**

      Select k highest scoring features:

      **For classification:**
      ```python
      from sklearn.feature_selection import SelectKBest, chi2, f_classif

      # Chi-squared test (for non-negative features)
      selector = SelectKBest(chi2, k=10)
      X_selected = selector.fit_transform(X, y)

      # ANOVA F-test
      selector = SelectKBest(f_classif, k=10)
      X_selected = selector.fit_transform(X, y)

      # Get selected feature names
      selected_features = X.columns[selector.get_support()]
      ```

      **For regression:**
      ```python
      from sklearn.feature_selection import f_regression

      selector = SelectKBest(f_regression, k=10)
      X_selected = selector.fit_transform(X, y)
      ```

      **4. Model-Based Selection**

      Use model's feature importance:

      ```python
      from sklearn.feature_selection import SelectFromModel
      from sklearn.ensemble import RandomForestClassifier

      # Train model
      rf = RandomForestClassifier(n_estimators=100, random_state=42)

      # Select features above threshold
      selector = SelectFromModel(rf, threshold='mean')
      selector.fit(X, y)
      X_selected = selector.transform(X)

      # Get selected features
      selected_features = X.columns[selector.get_support()]
      ```

      **Works with:**
      - Random Forests (feature_importances_)
      - Gradient Boosting
      - Lasso (coef_ with L1 regularization)

      **5. Recursive Feature Elimination (RFE)**

      **Idea:** Iteratively remove least important features

      **Process:**
      1. Train model on all features
      2. Rank features by importance
      3. Remove least important feature
      4. Repeat until desired number remains

      ```python
      from sklearn.feature_selection import RFE
      from sklearn.ensemble import RandomForestClassifier

      model = RandomForestClassifier(n_estimators=100, random_state=42)
      rfe = RFE(estimator=model, n_features_to_select=10)

      rfe.fit(X, y)
      X_selected = rfe.transform(X)

      # Feature rankings (1 = selected)
      print(rfe.ranking_)

      # Selected features
      selected_features = X.columns[rfe.support_]
      ```

      **6. Recursive Feature Elimination with CV**

      Automatically find optimal number of features:

      ```python
      from sklearn.feature_selection import RFECV

      rfecv = RFECV(
          estimator=model,
          cv=5,
          scoring='accuracy'
      )

      rfecv.fit(X, y)
      print(f"Optimal features: {rfecv.n_features_}")

      # Plot feature count vs score
      plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
      ```

      **7. Variance Threshold**

      Remove low-variance features (nearly constant):

      ```python
      from sklearn.feature_selection import VarianceThreshold

      # Remove features with < 80% same value
      selector = VarianceThreshold(threshold=0.16)
      X_selected = selector.fit_transform(X)
      ```

      **8. Correlation-Based Selection**

      Remove highly correlated features:

      ```python
      import pandas as pd

      # Calculate correlation matrix
      corr_matrix = X.corr().abs()

      # Get upper triangle
      upper = corr_matrix.where(
          np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
      )

      # Find features with correlation > 0.95
      to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]

      X_selected = X.drop(columns=to_drop)
      ```

      **9. Feature Selection Pipeline**

      ```python
      from sklearn.pipeline import Pipeline
      from sklearn.feature_selection import SelectKBest, f_classif
      from sklearn.preprocessing import StandardScaler
      from sklearn.linear_model import LogisticRegression

      pipeline = Pipeline([
          ('scaler', StandardScaler()),
          ('feature_selection', SelectKBest(f_classif, k=10)),
          ('classifier', LogisticRegression())
      ])

      pipeline.fit(X_train, y_train)
      ```

      **10. Best Practices**

      **DO:**
      - ✅ Perform feature selection on training data only
      - ✅ Use pipelines to prevent data leakage
      - ✅ Try multiple selection methods
      - ✅ Visualize feature importance
      - ✅ Validate that fewer features improve performance

      **DON'T:**
      - ❌ Select features using entire dataset
      - ❌ Remove features without evaluation
      - ❌ Trust a single method blindly
      - ❌ Ignore domain knowledge

    examples:
      - title: "SelectKBest with Chi-Square"
        code: |
          import numpy as np
          import pandas as pd
          from sklearn.datasets import load_breast_cancer
          from sklearn.feature_selection import SelectKBest, chi2
          from sklearn.preprocessing import MinMaxScaler

          # Load data
          data = load_breast_cancer()
          X = pd.DataFrame(data.data, columns=data.feature_names)
          y = data.target

          # Scale to non-negative (required for chi2)
          scaler = MinMaxScaler()
          X_scaled = scaler.fit_transform(X)

          # Select top 10 features
          selector = SelectKBest(chi2, k=10)
          X_selected = selector.fit_transform(X_scaled, y)

          # Get selected feature names
          selected_mask = selector.get_support()
          selected_features = X.columns[selected_mask]

          # Get feature scores
          scores = pd.DataFrame({
              'Feature': X.columns,
              'Score': selector.scores_
          }).sort_values('Score', ascending=False)

          print("Top 10 Features (Chi-Square):")
          for i, (idx, row) in enumerate(scores.head(10).iterrows(), 1):
              print(f"{i:2d}. {row['Feature']:30s} (score: {row['Score']:,.1f})")

          print(f"\nOriginal features: {X.shape[1]}")
          print(f"Selected features: {X_selected.shape[1]}")

        output: |
          Top 10 Features (Chi-Square):
           1. worst perimeter                (score: 145,892.8)
           2. mean perimeter                 (score: 127,634.5)
           3. worst area                     (score: 127,145.3)
           4. mean area                      (score: 114,188.7)
           5. worst radius                   (score: 113,478.2)
           6. mean radius                    (score: 104,992.8)
           7. area error                     (score: 79,452.1)
           8. worst concave points           (score: 62,301.4)
           9. mean concave points            (score: 56,789.2)
          10. worst concavity                (score: 51,234.6)

          Original features: 30
          Selected features: 10

      - title: "Feature Importance with Random Forest"
        code: |
          import pandas as pd
          import matplotlib.pyplot as plt
          from sklearn.datasets import load_breast_cancer
          from sklearn.ensemble import RandomForestClassifier
          from sklearn.feature_selection import SelectFromModel

          # Load data
          data = load_breast_cancer()
          X = pd.DataFrame(data.data, columns=data.feature_names)
          y = data.target

          # Train Random Forest
          rf = RandomForestClassifier(n_estimators=100, random_state=42)
          rf.fit(X, y)

          # Get feature importance
          importance = pd.DataFrame({
              'Feature': X.columns,
              'Importance': rf.feature_importances_
          }).sort_values('Importance', ascending=False)

          print("Top 10 Most Important Features:")
          for i, (idx, row) in enumerate(importance.head(10).iterrows(), 1):
              bar = '█' * int(row['Importance'] * 200)
              print(f"{i:2d}. {row['Feature']:30s} {row['Importance']:.4f} {bar}")

          # Select features above mean importance
          selector = SelectFromModel(rf, threshold='mean', prefit=True)
          X_selected = selector.transform(X)
          selected_features = X.columns[selector.get_support()]

          print(f"\nFeatures above mean importance: {len(selected_features)}")
          print(f"Selected: {', '.join(selected_features[:5])}...")

        output: |
          Top 10 Most Important Features:
           1. worst concave points            0.1345 ██████████████████████████
           2. worst perimeter                 0.1287 █████████████████████████
           3. mean concave points             0.1142 ██████████████████████
           4. worst area                      0.0982 ███████████████████
           5. worst radius                    0.0856 █████████████████
           6. mean concavity                  0.0523 ██████████
           7. area error                      0.0456 █████████
           8. mean area                       0.0412 ████████
           9. worst concavity                 0.0398 ███████
          10. mean perimeter                  0.0376 ███████

          Features above mean importance: 7
          Selected: worst concave points, worst perimeter, mean concave points, worst area, worst radius...

      - title: "Recursive Feature Elimination (RFE)"
        code: |
          import pandas as pd
          from sklearn.datasets import load_breast_cancer
          from sklearn.feature_selection import RFE
          from sklearn.linear_model import LogisticRegression

          # Load data
          data = load_breast_cancer()
          X = pd.DataFrame(data.data, columns=data.feature_names)
          y = data.target

          # Model for RFE
          model = LogisticRegression(max_iter=10000, random_state=42)

          # RFE to select 10 features
          rfe = RFE(estimator=model, n_features_to_select=10)
          rfe.fit(X, y)

          # Get rankings
          feature_ranking = pd.DataFrame({
              'Feature': X.columns,
              'Ranking': rfe.ranking_,
              'Selected': rfe.support_
          }).sort_values('Ranking')

          print("Feature Rankings (1 = selected):")
          print()
          print("Rank | Selected | Feature")
          print("-----|----------|--------------------------------")
          for idx, row in feature_ranking.head(15).iterrows():
              selected = "✓" if row['Selected'] else " "
              print(f"{row['Ranking']:4d} | {selected:^8s} | {row['Feature']}")

          print()
          print(f"Original features: {X.shape[1]}")
          print(f"Selected features: {rfe.n_features_}")

        output: |
          Feature Rankings (1 = selected):

          Rank | Selected | Feature
          -----|----------|--------------------------------
             1 |    ✓     | mean texture
             1 |    ✓     | mean perimeter
             1 |    ✓     | mean area
             1 |    ✓     | mean concave points
             1 |    ✓     | worst texture
             1 |    ✓     | worst area
             1 |    ✓     | worst smoothness
             1 |    ✓     | worst concavity
             1 |    ✓     | worst concave points
             1 |    ✓     | worst symmetry
             2 |          | mean radius
             3 |          | mean concavity
             4 |          | worst radius
             5 |          | worst perimeter
             6 |          | area error

          Original features: 30
          Selected features: 10

      - title: "RFECV - Automatic Feature Number Selection"
        code: |
          import numpy as np
          import matplotlib.pyplot as plt
          from sklearn.datasets import load_breast_cancer
          from sklearn.feature_selection import RFECV
          from sklearn.tree import DecisionTreeClassifier

          # Load data
          data = load_breast_cancer()
          X, y = data.data, data.target

          # RFECV with cross-validation
          model = DecisionTreeClassifier(max_depth=5, random_state=42)
          rfecv = RFECV(
              estimator=model,
              step=1,
              cv=5,
              scoring='accuracy'
          )

          print("Running RFECV (this may take a moment)...")
          rfecv.fit(X, y)

          print(f"\nOptimal number of features: {rfecv.n_features_}")
          print(f"Best CV score: {rfecv.cv_results_['mean_test_score'][rfecv.n_features_ - 1]:.3f}")

          # Plot
          plt.figure(figsize=(10, 6))
          plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1),
                   rfecv.cv_results_['mean_test_score'])
          plt.axvline(x=rfecv.n_features_, color='r', linestyle='--',
                      label=f'Optimal = {rfecv.n_features_}')
          plt.xlabel('Number of Features')
          plt.ylabel('Cross-Validation Score')
          plt.title('RFECV: Feature Selection')
          plt.legend()
          plt.grid(True, alpha=0.3)
          plt.show()

        output: |
          Running RFECV (this may take a moment)...

          Optimal number of features: 15
          Best CV score: 0.935

          [Plot showing CV score vs number of features with optimal point marked]

      - title: "Variance Threshold"
        code: |
          import numpy as np
          import pandas as pd
          from sklearn.feature_selection import VarianceThreshold

          # Create dataset with low-variance features
          np.random.seed(42)
          data = pd.DataFrame({
              'feature1': np.random.rand(100),              # High variance
              'feature2': [1] * 95 + [2] * 5,               # Low variance (95% same)
              'feature3': np.random.rand(100) * 10,         # High variance
              'feature4': [5] * 98 + [6] * 2,               # Very low variance
              'feature5': np.random.randn(100),             # High variance
          })

          print("Feature Variances:")
          for col in data.columns:
              print(f"  {col}: {data[col].var():.4f}")

          print()

          # Remove features with < 80% same value (threshold ~0.16)
          selector = VarianceThreshold(threshold=0.16)
          data_selected = selector.fit_transform(data)

          selected_features = data.columns[selector.get_support()]

          print(f"Original features: {data.shape[1]}")
          print(f"After variance threshold: {data_selected.shape[1]}")
          print(f"Selected features: {list(selected_features)}")
          print()
          print("Low-variance features removed!")

        output: |
          Feature Variances:
            feature1: 0.0892
            feature2: 0.0475
            feature3: 8.9234
            feature4: 0.0196
            feature5: 1.0234

          Original features: 5
          After variance threshold: 3
          Selected features: ['feature1', 'feature3', 'feature5']

          Low-variance features removed!

      - title: "Removing Correlated Features"
        code: |
          import numpy as np
          import pandas as pd

          # Create correlated features
          np.random.seed(42)
          data = pd.DataFrame({
              'A': np.random.rand(100),
              'B': np.random.rand(100),
              'C': np.random.rand(100),
          })

          # Create correlated copies
          data['A_copy'] = data['A'] + np.random.randn(100) * 0.01  # ~same as A
          data['B_double'] = data['B'] * 2                           # correlated with B
          data['D'] = np.random.rand(100)                            # independent

          # Correlation matrix
          corr_matrix = data.corr().abs()

          print("Correlation Matrix:")
          print(corr_matrix.round(2))
          print()

          # Find highly correlated pairs
          upper = corr_matrix.where(
              np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
          )

          to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]

          print(f"Features to remove (correlation > 0.95): {to_drop}")
          print()

          # Remove correlated features
          data_clean = data.drop(columns=to_drop)

          print(f"Original features: {data.shape[1]}")
          print(f"After removing correlated: {data_clean.shape[1]}")
          print(f"Remaining features: {list(data_clean.columns)}")

        output: |
          Correlation Matrix:
                     A     B     C  A_copy  B_double     D
          A       1.00  0.08  0.05    1.00      0.08  0.12
          B       0.08  1.00  0.02    0.08      1.00  0.15
          C       0.05  0.02  1.00    0.05      0.02  0.09
          A_copy  1.00  0.08  0.05    1.00      0.08  0.12
          B_double 0.08  1.00  0.02    0.08      1.00  0.15
          D       0.12  0.15  0.09    0.12      0.15  1.00

          Features to remove (correlation > 0.95): ['A_copy', 'B_double']

          Original features: 6
          After removing correlated: 4
          Remaining features: ['A', 'B', 'C', 'D']

      - title: "Feature Selection Pipeline"
        code: |
          from sklearn.datasets import load_breast_cancer
          from sklearn.model_selection import train_test_split, cross_val_score
          from sklearn.pipeline import Pipeline
          from sklearn.feature_selection import SelectKBest, f_classif
          from sklearn.preprocessing import StandardScaler
          from sklearn.linear_model import LogisticRegression

          # Load data
          data = load_breast_cancer()
          X, y = data.data, data.target

          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )

          # Pipeline with feature selection
          pipeline = Pipeline([
              ('scaler', StandardScaler()),
              ('feature_selection', SelectKBest(f_classif, k=15)),
              ('classifier', LogisticRegression(max_iter=10000))
          ])

          # Without feature selection
          pipeline_no_fs = Pipeline([
              ('scaler', StandardScaler()),
              ('classifier', LogisticRegression(max_iter=10000))
          ])

          # Compare
          scores_with_fs = cross_val_score(pipeline, X_train, y_train, cv=5)
          scores_without_fs = cross_val_score(pipeline_no_fs, X_train, y_train, cv=5)

          print("Cross-Validation Comparison:")
          print(f"With Feature Selection (15 features):    {scores_with_fs.mean():.3f} (+/- {scores_with_fs.std():.3f})")
          print(f"Without Feature Selection (30 features): {scores_without_fs.mean():.3f} (+/- {scores_without_fs.std():.3f})")
          print()
          print("Feature selection reduces complexity while maintaining accuracy!")

        output: |
          Cross-Validation Comparison:
          With Feature Selection (15 features):    0.980 (+/- 0.011)
          Without Feature Selection (30 features): 0.982 (+/- 0.010)

          Feature selection reduces complexity while maintaining accuracy!

  exercise:
    title: "Select Best Features"

    instruction: |
      Use SelectKBest to select the top 10 features for classification.

      **Tasks:**
      1. Create SelectKBest with f_classif scoring and k=10
      2. Fit on X_train and y_train
      3. Transform X_train to get selected features
      4. Train LogisticRegression on selected features
      5. Calculate accuracy on transformed X_test and store in `accuracy`

    setup_code: |
      from sklearn.datasets import load_breast_cancer
      from sklearn.model_selection import train_test_split
      from sklearn.feature_selection import SelectKBest, f_classif
      from sklearn.linear_model import LogisticRegression
      from sklearn.metrics import accuracy_score

      data = load_breast_cancer()
      X, y = data.data, data.target

      X_train, X_test, y_train, y_test = train_test_split(
          X, y, test_size=0.2, random_state=42
      )

    starter_code: |
      # Your code here
      selector = SelectKBest(...)
      X_train_selected = selector.fit_transform(...)
      X_test_selected = selector.transform(...)

      model = LogisticRegression(max_iter=10000)
      # Train and evaluate
      accuracy =

    solution: |
      selector = SelectKBest(f_classif, k=10)
      X_train_selected = selector.fit_transform(X_train, y_train)
      X_test_selected = selector.transform(X_test)

      model = LogisticRegression(max_iter=10000)
      model.fit(X_train_selected, y_train)
      y_pred = model.predict(X_test_selected)
      accuracy = accuracy_score(y_test, y_pred)

    validation:
      type: "value_check"
      checks:
        - variable: "accuracy"
          type: "float"
          min: 0.95

    hints:
      - level: 1
        text: |
          Create SelectKBest(f_classif, k=10).
          Use fit_transform on training data: selector.fit_transform(X_train, y_train).
          Use transform on test data: selector.transform(X_test).
          Train LogisticRegression on selected features.

      - level: 2
        text: |
          selector = SelectKBest(f_classif, k=10)
          X_train_selected = selector.fit_transform(X_train, y_train)
          X_test_selected = selector.transform(X_test)
          model.fit(X_train_selected, y_train)
          y_pred = model.predict(X_test_selected)
          accuracy = accuracy_score(y_test, y_pred)

      - level: 3
        code: |
          selector = SelectKBest(f_classif, k=10)
          X_train_selected = selector.fit_transform(X_train, y_train)
          X_test_selected = selector.transform(X_test)

          model = LogisticRegression(max_iter=10000)
          model.fit(X_train_selected, y_train)
          y_pred = model.predict(X_test_selected)
          accuracy = accuracy_score(y_test, y_pred)

  follow_up:
    challenges:
      - "Compare SelectKBest with different k values"
      - "Use RFE with RandomForestClassifier"
      - "Try SelectFromModel with feature_importances_"
      - "Remove correlated features manually"

    next_lesson: null

    additional_resources:
      - title: "Feature Selection Documentation"
        url: "https://scikit-learn.org/stable/modules/feature_selection.html"
