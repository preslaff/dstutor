lesson:
  id: "sklearn_09"
  level: "intermediate"
  topic: "sklearn"
  subtopic: "Preprocessing & Scaling"
  order: 9

  metadata:
    duration: "35 min"
    difficulty: "medium"
    prerequisites: ["sklearn_02", "sklearn_03"]
    learning_objectives:
      - "Understand why scaling is important"
      - "Use StandardScaler and MinMaxScaler"
      - "Encode categorical variables properly"
      - "Build preprocessing pipelines"

  content:
    introduction: |
      # Preprocessing & Scaling

      Clean and transform your data for optimal model performance! Learn essential
      preprocessing techniques that can dramatically improve your results.

      **What you'll learn:**
      - Feature scaling methods
      - Categorical encoding
      - Handling missing values
      - Preprocessing pipelines

    concept: |
      ## Preparing Data for Machine Learning

      **1. Why Preprocessing Matters**

      **Problem:** Different scales affect models
      - Feature 1: Age (20-80)
      - Feature 2: Income ($20,000-$200,000)
      - Income dominates due to scale!

      **Affected models:**
      - ❌ Linear Regression, Logistic Regression
      - ❌ SVM, KNN, Neural Networks

      **Not affected:**
      - ✅ Tree-based models (Decision Trees, Random Forests)

      **2. Standard Scaling (Standardization)**

      **Formula:** z = (x - mean) / std

      **Result:**
      - Mean = 0
      - Standard deviation = 1
      - Preserves outliers
      - Most common choice

      ```python
      from sklearn.preprocessing import StandardScaler

      scaler = StandardScaler()
      X_train_scaled = scaler.fit_transform(X_train)
      X_test_scaled = scaler.transform(X_test)  # Use same parameters!
      ```

      **CRITICAL: Fit on training data only!**
      - Fit scaler on X_train
      - Transform both X_train and X_test
      - Prevents data leakage

      **3. Min-Max Scaling (Normalization)**

      **Formula:** x_scaled = (x - min) / (max - min)

      **Result:**
      - Range: 0 to 1
      - Preserves original distribution
      - Sensitive to outliers

      ```python
      from sklearn.preprocessing import MinMaxScaler

      scaler = MinMaxScaler()
      X_train_scaled = scaler.fit_transform(X_train)
      X_test_scaled = scaler.transform(X_test)
      ```

      **When to use:**
      - Need bounded range (0-1)
      - Neural networks (bounded activations)
      - Image data normalization

      **4. Robust Scaling**

      **Formula:** x_scaled = (x - median) / IQR

      **Result:**
      - Uses median and IQR (Interquartile Range)
      - Resistant to outliers
      - Centers around 0

      ```python
      from sklearn.preprocessing import RobustScaler

      scaler = RobustScaler()
      X_train_scaled = scaler.fit_transform(X_train)
      X_test_scaled = scaler.transform(X_test)
      ```

      **5. Encoding Categorical Variables**

      **Label Encoding** (for ordinal data):
      ```python
      from sklearn.preprocessing import LabelEncoder

      le = LabelEncoder()
      df['size_encoded'] = le.fit_transform(df['size'])
      # 'small'=0, 'medium'=1, 'large'=2
      ```

      **One-Hot Encoding** (for nominal data):
      ```python
      from sklearn.preprocessing import OneHotEncoder

      encoder = OneHotEncoder(sparse_output=False, drop='first')
      encoded = encoder.fit_transform(df[['color']])
      # color_blue, color_green, color_red → binary columns
      ```

      **Ordinal Encoding** (custom order):
      ```python
      from sklearn.preprocessing import OrdinalEncoder

      encoder = OrdinalEncoder(
          categories=[['low', 'medium', 'high']]
      )
      df['priority_encoded'] = encoder.fit_transform(df[['priority']])
      ```

      **6. Handling Missing Values**

      ```python
      from sklearn.impute import SimpleImputer

      # Numerical: mean/median
      imputer = SimpleImputer(strategy='mean')
      X_imputed = imputer.fit_transform(X)

      # Categorical: most_frequent
      imputer = SimpleImputer(strategy='most_frequent')
      X_imputed = imputer.fit_transform(X)
      ```

      **7. Preprocessing Pipelines**

      Chain transformations together:
      ```python
      from sklearn.pipeline import Pipeline
      from sklearn.preprocessing import StandardScaler
      from sklearn.linear_model import LogisticRegression

      pipeline = Pipeline([
          ('scaler', StandardScaler()),
          ('classifier', LogisticRegression())
      ])

      pipeline.fit(X_train, y_train)
      predictions = pipeline.predict(X_test)
      ```

      **Advantages:**
      - ✅ Prevents data leakage
      - ✅ Cleaner code
      - ✅ Easy to save/load
      - ✅ Consistent preprocessing

      **8. Column Transformer**

      Different preprocessing for different columns:
      ```python
      from sklearn.compose import ColumnTransformer
      from sklearn.preprocessing import StandardScaler, OneHotEncoder

      preprocessor = ColumnTransformer([
          ('num', StandardScaler(), ['age', 'salary']),
          ('cat', OneHotEncoder(), ['city', 'department'])
      ])

      X_transformed = preprocessor.fit_transform(X)
      ```

    examples:
      - title: "StandardScaler - Before and After"
        code: |
          import numpy as np
          import pandas as pd
          from sklearn.preprocessing import StandardScaler

          # Create data with different scales
          data = pd.DataFrame({
              'Age': [25, 30, 35, 40, 45],
              'Salary': [40000, 50000, 60000, 70000, 80000],
              'Years_Exp': [2, 5, 8, 12, 15]
          })

          print("Original Data:")
          print(data)
          print("\nOriginal Statistics:")
          print(data.describe())
          print()

          # Scale the data
          scaler = StandardScaler()
          scaled_data = scaler.fit_transform(data)
          scaled_df = pd.DataFrame(
              scaled_data,
              columns=data.columns
          )

          print("Scaled Data:")
          print(scaled_df)
          print("\nScaled Statistics:")
          print(scaled_df.describe())

        output: |
          Original Data:
             Age  Salary  Years_Exp
          0   25   40000          2
          1   30   50000          5
          2   35   60000          8
          3   40   70000         12
          4   45   80000         15

          Original Statistics:
                   Age        Salary  Years_Exp
          mean    35.0      60000.00       8.40
          std      7.91     15811.39       5.18

          Scaled Data:
                Age    Salary  Years_Exp
          0  -1.265    -1.265     -1.236
          1  -0.632    -0.632     -0.656
          2   0.000     0.000      0.077
          3   0.632     0.632      0.695
          4   1.265     1.265      1.274

          Scaled Statistics:
                   Age    Salary  Years_Exp
          mean    0.00      0.00       0.00
          std     1.00      1.00       1.00

      - title: "Comparing Scaling Methods"
        code: |
          import numpy as np
          from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

          # Data with outlier
          X = np.array([[1], [2], [3], [4], [5], [100]])

          # Standard Scaling
          standard_scaler = StandardScaler()
          X_standard = standard_scaler.fit_transform(X)

          # Min-Max Scaling
          minmax_scaler = MinMaxScaler()
          X_minmax = minmax_scaler.fit_transform(X)

          # Robust Scaling
          robust_scaler = RobustScaler()
          X_robust = robust_scaler.fit_transform(X)

          print("Original | Standard | MinMax  | Robust")
          print("---------|----------|---------|--------")
          for i in range(len(X)):
              print(f"{X[i, 0]:8.0f} | {X_standard[i, 0]:8.3f} | "
                    f"{X_minmax[i, 0]:7.3f} | {X_robust[i, 0]:7.3f}")

        output: |
          Original | Standard | MinMax  | Robust
          ---------|----------|---------|--------
                 1 |   -0.355 |   0.000 |  -0.500
                 2 |   -0.331 |   0.010 |  -0.333
                 3 |   -0.307 |   0.020 |  -0.167
                 4 |   -0.283 |   0.030 |   0.000
                 5 |   -0.259 |   0.040 |   0.167
               100 |    2.534 |   1.000 |  16.000

      - title: "Impact of Scaling on Model Performance"
        code: |
          import numpy as np
          from sklearn.datasets import load_breast_cancer
          from sklearn.model_selection import train_test_split
          from sklearn.linear_model import LogisticRegression
          from sklearn.preprocessing import StandardScaler
          from sklearn.metrics import accuracy_score

          # Load data
          data = load_breast_cancer()
          X, y = data.data, data.target

          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )

          # Without scaling
          model_unscaled = LogisticRegression(max_iter=10000)
          model_unscaled.fit(X_train, y_train)
          y_pred_unscaled = model_unscaled.predict(X_test)
          acc_unscaled = accuracy_score(y_test, y_pred_unscaled)

          # With scaling
          scaler = StandardScaler()
          X_train_scaled = scaler.fit_transform(X_train)
          X_test_scaled = scaler.transform(X_test)

          model_scaled = LogisticRegression(max_iter=10000)
          model_scaled.fit(X_train_scaled, y_train)
          y_pred_scaled = model_scaled.predict(X_test_scaled)
          acc_scaled = accuracy_score(y_test, y_pred_scaled)

          print("Logistic Regression Performance:")
          print(f"  Without Scaling: {acc_unscaled:.3f}")
          print(f"  With Scaling:    {acc_scaled:.3f}")
          print(f"  Improvement:     {(acc_scaled - acc_unscaled):.3f}")

        output: |
          Logistic Regression Performance:
            Without Scaling: 0.974
            With Scaling:    0.982
            Improvement:     0.008

      - title: "Label Encoding vs One-Hot Encoding"
        code: |
          import pandas as pd
          from sklearn.preprocessing import LabelEncoder, OneHotEncoder

          # Sample data
          df = pd.DataFrame({
              'City': ['NYC', 'LA', 'Chicago', 'NYC', 'LA'],
              'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small']
          })

          print("Original Data:")
          print(df)
          print()

          # Label Encoding (ordinal)
          le = LabelEncoder()
          df['Size_Encoded'] = le.fit_transform(df['Size'])

          print("Label Encoding (Size):")
          print(df[['Size', 'Size_Encoded']])
          print()

          # One-Hot Encoding (nominal)
          encoder = OneHotEncoder(sparse_output=False, drop='first')
          city_encoded = encoder.fit_transform(df[['City']])
          city_encoded_df = pd.DataFrame(
              city_encoded,
              columns=encoder.get_feature_names_out(['City'])
          )

          print("One-Hot Encoding (City):")
          print(pd.concat([df[['City']], city_encoded_df], axis=1))

        output: |
          Original Data:
               City    Size
          0     NYC   Small
          1      LA  Medium
          2 Chicago   Large
          3     NYC  Medium
          4      LA   Small

          Label Encoding (Size):
              Size  Size_Encoded
          0  Small             2
          1 Medium             1
          2  Large             0
          3 Medium             1
          4  Small             2

          One-Hot Encoding (City):
               City  City_LA  City_NYC
          0     NYC      0.0       1.0
          1      LA      1.0       0.0
          2 Chicago      0.0       0.0
          3     NYC      0.0       1.0
          4      LA      1.0       0.0

      - title: "Handling Missing Values"
        code: |
          import numpy as np
          import pandas as pd
          from sklearn.impute import SimpleImputer

          # Data with missing values
          data = pd.DataFrame({
              'Age': [25, np.nan, 35, 40, np.nan],
              'Salary': [50000, 60000, np.nan, 70000, 55000],
              'City': ['NYC', 'LA', None, 'Chicago', 'NYC']
          })

          print("Original Data with Missing Values:")
          print(data)
          print(f"\nMissing values per column:")
          print(data.isnull().sum())
          print()

          # Impute numerical columns with mean
          num_imputer = SimpleImputer(strategy='mean')
          data[['Age', 'Salary']] = num_imputer.fit_transform(
              data[['Age', 'Salary']]
          )

          # Impute categorical with most frequent
          cat_imputer = SimpleImputer(strategy='most_frequent')
          data[['City']] = cat_imputer.fit_transform(data[['City']])

          print("After Imputation:")
          print(data)
          print(f"\nMissing values per column:")
          print(data.isnull().sum())

        output: |
          Original Data with Missing Values:
              Age   Salary     City
          0  25.0  50000.0      NYC
          1   NaN  60000.0       LA
          2  35.0      NaN     None
          3  40.0  70000.0  Chicago
          4   NaN  55000.0      NYC

          Missing values per column:
          Age       2
          Salary    1
          City      1
          dtype: int64

          After Imputation:
              Age   Salary     City
          0  25.0  50000.0      NYC
          1  33.3  60000.0       LA
          2  35.0  58750.0      NYC
          3  40.0  70000.0  Chicago
          4  33.3  55000.0      NYC

          Missing values per column:
          Age       0
          Salary    0
          City      0
          dtype: int64

      - title: "Preprocessing Pipeline"
        code: |
          import numpy as np
          from sklearn.datasets import load_breast_cancer
          from sklearn.model_selection import train_test_split
          from sklearn.pipeline import Pipeline
          from sklearn.preprocessing import StandardScaler
          from sklearn.linear_model import LogisticRegression
          from sklearn.metrics import accuracy_score

          # Load data
          data = load_breast_cancer()
          X, y = data.data, data.target

          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )

          # Create pipeline
          pipeline = Pipeline([
              ('scaler', StandardScaler()),
              ('classifier', LogisticRegression(max_iter=10000))
          ])

          # Train (scaling happens automatically)
          pipeline.fit(X_train, y_train)

          # Predict (scaling happens automatically)
          y_pred = pipeline.predict(X_test)

          # Evaluate
          accuracy = accuracy_score(y_test, y_pred)

          print("Pipeline Components:")
          for name, step in pipeline.named_steps.items():
              print(f"  {name}: {type(step).__name__}")
          print()
          print(f"Accuracy: {accuracy:.3f}")
          print()
          print("Pipeline prevents data leakage and simplifies workflow!")

        output: |
          Pipeline Components:
            scaler: StandardScaler
            classifier: LogisticRegression

          Accuracy: 0.982

          Pipeline prevents data leakage and simplifies workflow!

      - title: "Column Transformer for Mixed Data"
        code: |
          import pandas as pd
          import numpy as np
          from sklearn.compose import ColumnTransformer
          from sklearn.preprocessing import StandardScaler, OneHotEncoder
          from sklearn.pipeline import Pipeline
          from sklearn.linear_model import LogisticRegression

          # Create mixed dataset
          np.random.seed(42)
          data = pd.DataFrame({
              'Age': np.random.randint(25, 60, 100),
              'Salary': np.random.randint(40000, 120000, 100),
              'City': np.random.choice(['NYC', 'LA', 'Chicago'], 100),
              'Department': np.random.choice(['IT', 'Sales', 'HR'], 100),
              'Left': np.random.choice([0, 1], 100)
          })

          # Separate features and target
          X = data.drop('Left', axis=1)
          y = data['Left']

          # Define preprocessing for different column types
          numeric_features = ['Age', 'Salary']
          categorical_features = ['City', 'Department']

          preprocessor = ColumnTransformer([
              ('num', StandardScaler(), numeric_features),
              ('cat', OneHotEncoder(drop='first'), categorical_features)
          ])

          # Create pipeline
          pipeline = Pipeline([
              ('preprocessor', preprocessor),
              ('classifier', LogisticRegression())
          ])

          # Train
          pipeline.fit(X, y)

          print("Mixed Data Preprocessing:")
          print(f"  Numeric features: {numeric_features}")
          print(f"    → StandardScaler")
          print(f"  Categorical features: {categorical_features}")
          print(f"    → OneHotEncoder")
          print()
          print("Pipeline handles different column types automatically!")

        output: |
          Mixed Data Preprocessing:
            Numeric features: ['Age', 'Salary']
              → StandardScaler
            Categorical features: ['City', 'Department']
              → OneHotEncoder

          Pipeline handles different column types automatically!

  exercise:
    title: "Scale Features for Better Performance"

    instruction: |
      Apply StandardScaler to improve model performance.

      **Tasks:**
      1. Create a StandardScaler
      2. Fit the scaler on X_train and transform it
      3. Transform X_test (without fitting again!)
      4. Train LogisticRegression on scaled data
      5. Calculate accuracy on scaled test set and store in `accuracy`

    setup_code: |
      import numpy as np
      from sklearn.datasets import load_breast_cancer
      from sklearn.model_selection import train_test_split
      from sklearn.preprocessing import StandardScaler
      from sklearn.linear_model import LogisticRegression
      from sklearn.metrics import accuracy_score

      data = load_breast_cancer()
      X, y = data.data, data.target

      X_train, X_test, y_train, y_test = train_test_split(
          X, y, test_size=0.2, random_state=42
      )

    starter_code: |
      # Your code here
      scaler = StandardScaler()
      # Scale the data
      X_train_scaled =
      X_test_scaled =

      # Train model
      model = LogisticRegression(max_iter=10000)
      # Calculate accuracy
      accuracy =

    solution: |
      scaler = StandardScaler()
      X_train_scaled = scaler.fit_transform(X_train)
      X_test_scaled = scaler.transform(X_test)

      model = LogisticRegression(max_iter=10000)
      model.fit(X_train_scaled, y_train)
      y_pred = model.predict(X_test_scaled)
      accuracy = accuracy_score(y_test, y_pred)

    validation:
      type: "value_check"
      checks:
        - variable: "accuracy"
          type: "float"
          min: 0.97

    hints:
      - level: 1
        text: |
          Use scaler.fit_transform(X_train) to fit and transform training data.
          Use scaler.transform(X_test) for test data (no fitting!).
          Train LogisticRegression on scaled data.
          Calculate accuracy with accuracy_score(y_test, y_pred).

      - level: 2
        text: |
          X_train_scaled = scaler.fit_transform(X_train)
          X_test_scaled = scaler.transform(X_test)
          model.fit(X_train_scaled, y_train)
          y_pred = model.predict(X_test_scaled)
          accuracy = accuracy_score(y_test, y_pred)

      - level: 3
        code: |
          scaler = StandardScaler()
          X_train_scaled = scaler.fit_transform(X_train)
          X_test_scaled = scaler.transform(X_test)

          model = LogisticRegression(max_iter=10000)
          model.fit(X_train_scaled, y_train)
          y_pred = model.predict(X_test_scaled)
          accuracy = accuracy_score(y_test, y_pred)

  follow_up:
    challenges:
      - "Compare StandardScaler vs MinMaxScaler performance"
      - "Create a preprocessing pipeline"
      - "Handle missing values with SimpleImputer"
      - "Use ColumnTransformer for mixed data types"

    next_lesson: "sklearn_10"

    additional_resources:
      - title: "Preprocessing Documentation"
        url: "https://scikit-learn.org/stable/modules/preprocessing.html"
