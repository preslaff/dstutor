lesson:
  id: "preprocessing_03"
  level: "intermediate"
  topic: "preprocessing"
  subtopic: "Encoding Categorical Variables"
  order: 3

  metadata:
    duration: "40 min"
    difficulty: "medium"
    prerequisites:
      - "pandas_01"
      - "eda_06"
      - "sklearn_01"
    learning_objectives:
      - "Apply different categorical encoding techniques (label, one-hot, ordinal)"
      - "Choose appropriate encoding based on cardinality and model requirements"
      - "Handle high-cardinality categorical variables"
      - "Avoid common encoding pitfalls and data leakage"

  content:
    introduction: |
      # Encoding Categorical Variables

      Most machine learning algorithms require numerical input. Encoding converts categorical variables into numerical representations that algorithms can process.

      **Why This Matters:**
      - ML models can't directly handle strings
      - Different encodings capture different information
      - Wrong encoding can hurt model performance
      - Encoding strategy affects model interpretation

      **Key Decision:**
      Which encoding preserves the most useful information for your specific problem?

    concept: |
      ## Categorical Encoding Methods

      ### 1. Label Encoding

      **How it Works:**
      - Assigns integer to each category: A→0, B→1, C→2

      **When to Use:**
      - Tree-based models (Random Forest, XGBoost)
      - Ordinal categories with natural order
      - Binary categories (Yes/No → 0/1)

      **Pros:**
      - Simple and memory efficient
      - Preserves cardinality
      - Works well with tree models

      **Cons:**
      - Implies ordering (A < B < C)
      - Not suitable for linear models
      - Can mislead distance-based algorithms

      ### 2. One-Hot Encoding

      **How it Works:**
      - Creates binary column for each category
      - Example: Color=[Red, Blue] → Color_Red=[1,0], Color_Blue=[0,1]

      **When to Use:**
      - Nominal categories (no order)
      - Linear models, neural networks
      - Low-to-medium cardinality (<10-20 categories)

      **Pros:**
      - No ordinal assumption
      - Preserves all category information
      - Works with most algorithms

      **Cons:**
      - Increases dimensionality
      - Memory intensive for high cardinality
      - Can cause multicollinearity

      ### 3. Ordinal Encoding

      **How it Works:**
      - Assigns integers based on meaningful order
      - Example: Size: Small→1, Medium→2, Large→3

      **When to Use:**
      - Categories have clear ordering
      - Want to preserve ordinal relationship
      - All model types

      **Pros:**
      - Preserves order information
      - Memory efficient
      - Meaningful to all algorithms

      **Cons:**
      - Assumes equal spacing between categories
      - Requires domain knowledge
      - Only for ordinal data

      ### 4. Frequency/Count Encoding

      **How it Works:**
      - Replaces category with its frequency/count
      - Example: A appears 100 times → A→100

      **When to Use:**
      - High cardinality features
      - Frequency is informative
      - Want dimension reduction

      **Pros:**
      - Handles high cardinality
      - Single column output
      - Captures popularity

      **Cons:**
      - Loses category identity
      - Different categories may have same frequency
      - Can leak information if not careful

      ### 5. Target Encoding (Mean Encoding)

      **How it Works:**
      - Replaces category with mean target value for that category
      - Example: Category A has mean target = 0.75 → A→0.75

      **When to Use:**
      - High cardinality features
      - Strong relationship with target
      - Tree-based models

      **Pros:**
      - Powerful signal
      - Handles high cardinality
      - Single column

      **Cons:**
      - **Risk of overfitting**
      - **Data leakage risk**
      - Requires careful cross-validation
      - Only for supervised learning

      ### 6. Binary Encoding

      **How it Works:**
      - Converts categories to binary digits
      - Example: 5 categories → 3 binary columns (2^3=8)

      **When to Use:**
      - High cardinality (middle ground between label and one-hot)
      - Want dimension reduction
      - Tree-based models

      **Pros:**
      - Less dimensions than one-hot
      - Handles high cardinality
      - No ordinal assumption

      **Cons:**
      - Less interpretable
      - Arbitrary binary representation

    examples:
      - title: "Example 1: Label Encoding"
        code: |
          import pandas as pd
          from sklearn.preprocessing import LabelEncoder

          df = pd.DataFrame({
              'color': ['Red', 'Blue', 'Green', 'Red', 'Blue'],
              'size': ['S', 'M', 'L', 'M', 'S']
          })

          print("Original data:")
          print(df)

          # Label encoding
          le_color = LabelEncoder()
          df['color_encoded'] = le_color.fit_transform(df['color'])

          le_size = LabelEncoder()
          df['size_encoded'] = le_size.fit_transform(df['size'])

          print("\nAfter label encoding:")
          print(df)

          print("\nColor mapping:", dict(zip(le_color.classes_, le_color.transform(le_color.classes_))))
          print("Size mapping:", dict(zip(le_size.classes_, le_size.transform(le_size.classes_))))
        output: |
          Original data:
            color size
          0   Red    S
          1  Blue    M
          2 Green    L
          3   Red    M
          4  Blue    S

          After label encoding:
            color size  color_encoded  size_encoded
          0   Red    S              2             2
          1  Blue    M              0             1
          2 Green    L              1             0
          3   Red    M              2             1
          4  Blue    S              0             2

          Color mapping: {'Blue': 0, 'Green': 1, 'Red': 2}
          Size mapping: {'L': 0, 'M': 1, 'S': 2}

      - title: "Example 2: One-Hot Encoding"
        code: |
          import pandas as pd

          df = pd.DataFrame({
              'city': ['NYC', 'LA', 'Chicago', 'NYC', 'LA'],
              'category': ['A', 'B', 'A', 'C', 'B']
          })

          print("Original data:")
          print(df)
          print(f"Shape: {df.shape}")

          # One-hot encoding
          df_encoded = pd.get_dummies(df, columns=['city', 'category'], drop_first=False)

          print("\nAfter one-hot encoding:")
          print(df_encoded)
          print(f"Shape: {df_encoded.shape}")

          # With drop_first=True (avoids dummy variable trap)
          df_encoded_drop = pd.get_dummies(df, columns=['city', 'category'], drop_first=True)

          print("\nWith drop_first=True:")
          print(df_encoded_drop)
          print(f"Shape: {df_encoded_drop.shape}")
        output: |
          Original data:
               city category
          0    NYC        A
          1     LA        B
          2 Chicago        A
          3    NYC        C
          4     LA        B
          Shape: (5, 2)

          After one-hot encoding:
             city_Chicago  city_LA  city_NYC  category_A  category_B  category_C
          0             0        0         1           1           0           0
          1             0        1         0           0           1           0
          2             1        0         0           1           0           0
          3             0        0         1           0           0           1
          4             0        1         0           0           1           0
          Shape: (5, 6)

          With drop_first=True:
             city_LA  city_NYC  category_B  category_C
          0        0         1           0           0
          1        1         0           1           0
          2        0         0           0           0
          3        0         1           0           1
          4        1         0           1           0
          Shape: (5, 4)

      - title: "Example 3: Ordinal Encoding"
        code: |
          import pandas as pd
          from sklearn.preprocessing import OrdinalEncoder

          df = pd.DataFrame({
              'education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor'],
              'satisfaction': ['Low', 'Medium', 'High', 'Medium', 'Low']
          })

          print("Original data:")
          print(df)

          # Define ordinal mappings
          education_order = ['High School', 'Bachelor', 'Master', 'PhD']
          satisfaction_order = ['Low', 'Medium', 'High']

          # Ordinal encoding
          ordinal_encoder = OrdinalEncoder(
              categories=[education_order, satisfaction_order]
          )

          df[['education_encoded', 'satisfaction_encoded']] = ordinal_encoder.fit_transform(df)

          print("\nAfter ordinal encoding:")
          print(df)

          print("\nEducation mapping:")
          for i, cat in enumerate(education_order):
              print(f"  {cat}: {i}")

          print("\nSatisfaction mapping:")
          for i, cat in enumerate(satisfaction_order):
              print(f"  {cat}: {i}")
        output: |
          Original data:
              education satisfaction
          0 High School          Low
          1    Bachelor       Medium
          2      Master         High
          3         PhD       Medium
          4    Bachelor          Low

          After ordinal encoding:
              education satisfaction  education_encoded  satisfaction_encoded
          0 High School          Low                0.0                   0.0
          1    Bachelor       Medium                1.0                   1.0
          2      Master         High                2.0                   2.0
          3         PhD       Medium                3.0                   1.0
          4    Bachelor          Low                1.0                   0.0

          Education mapping:
            High School: 0
            Bachelor: 1
            Master: 2
            PhD: 3

          Satisfaction mapping:
            Low: 0
            Medium: 1
            High: 2

      - title: "Example 4: Frequency Encoding"
        code: |
          import pandas as pd

          df = pd.DataFrame({
              'city': ['NYC', 'LA', 'NYC', 'Chicago', 'NYC', 'LA', 'NYC', 'Chicago']
          })

          print("Original data:")
          print(df)

          # Calculate frequency
          frequency_map = df['city'].value_counts().to_dict()
          print("\nFrequency map:", frequency_map)

          # Apply frequency encoding
          df['city_frequency'] = df['city'].map(frequency_map)

          print("\nAfter frequency encoding:")
          print(df)

          print("\nNYC appears most frequently, gets highest value")
        output: |
          Original data:
               city
          0    NYC
          1     LA
          2    NYC
          3 Chicago
          4    NYC
          5     LA
          6    NYC
          7 Chicago

          Frequency map: {'NYC': 4, 'LA': 2, 'Chicago': 2}

          After frequency encoding:
               city  city_frequency
          0    NYC               4
          1     LA               2
          2    NYC               4
          3 Chicago               2
          4    NYC               4
          5     LA               2
          6    NYC               4
          7 Chicago               2

          NYC appears most frequently, gets highest value

      - title: "Example 5: Target Encoding (with proper CV)"
        code: |
          import pandas as pd
          import numpy as np

          # Create data with category-target relationship
          np.random.seed(42)
          df = pd.DataFrame({
              'category': ['A']*30 + ['B']*30 + ['C']*40,
              'target': np.concatenate([
                  np.random.choice([0, 1], 30, p=[0.3, 0.7]),  # A: 70% positive
                  np.random.choice([0, 1], 30, p=[0.6, 0.4]),  # B: 40% positive
                  np.random.choice([0, 1], 40, p=[0.8, 0.2])   # C: 20% positive
              ])
          })

          print("Original data summary:")
          print(df.groupby('category')['target'].agg(['mean', 'count']))

          # Target encoding (mean target by category)
          target_means = df.groupby('category')['target'].mean()
          print("\nTarget encoding map:")
          print(target_means)

          df['category_target_encoded'] = df['category'].map(target_means)

          print("\nAfter target encoding:")
          print(df.head(10))

          print("\n⚠️  WARNING: Use proper cross-validation to avoid overfitting!")
          print("This is simplified for demonstration.")
        output: |
          Original data summary:
                   mean  count
          category
          A        0.70     30
          B        0.40     30
          C        0.20     40

          Target encoding map:
          category
          A    0.70
          B    0.40
          C    0.20
          Name: target, dtype: float64

          After target encoding:
             category  target  category_target_encoded
          0         A       1                     0.70
          1         A       1                     0.70
          2         A       0                     0.70
          3         A       1                     0.70
          4         A       1                     0.70
          5         A       1                     0.70
          6         A       1                     0.70
          7         A       0                     0.70
          8         A       1                     0.70
          9         A       1                     0.70

          ⚠️  WARNING: Use proper cross-validation to avoid overfitting!
          This is simplified for demonstration.

      - title: "Example 6: Handling High Cardinality"
        code: |
          import pandas as pd
          import numpy as np

          # High cardinality categorical
          np.random.seed(42)
          n_categories = 100
          categories = [f'Cat_{i}' for i in range(n_categories)]
          df = pd.DataFrame({
              'high_cardinality': np.random.choice(categories, 1000)
          })

          print(f"Original cardinality: {df['high_cardinality'].nunique()}")
          print(f"Original shape: {df.shape}")

          # Strategy 1: Group rare categories
          value_counts = df['high_cardinality'].value_counts()
          threshold = 10  # Minimum frequency
          rare_categories = value_counts[value_counts < threshold].index

          df['grouped'] = df['high_cardinality'].apply(
              lambda x: 'Other' if x in rare_categories else x
          )
          print(f"\nAfter grouping rare categories:")
          print(f"New cardinality: {df['grouped'].nunique()}")

          # Strategy 2: Frequency encoding
          freq_map = df['high_cardinality'].value_counts().to_dict()
          df['frequency_encoded'] = df['high_cardinality'].map(freq_map)
          print(f"\nFrequency encoding shape: {df[['frequency_encoded']].shape}")

          # Strategy 3: One-hot would create 100 columns!
          print(f"\n⚠️  One-hot encoding would create {n_categories} columns!")
          print("Use frequency/target encoding or grouping instead")
        output: |
          Original cardinality: 100
          Original shape: (1000, 1)

          After grouping rare categories:
          New cardinality: 42

          Frequency encoding shape: (1000, 1)

          ⚠️  One-hot encoding would create 100 columns!
          Use frequency/target encoding or grouping instead

      - title: "Example 7: Encoding in Train/Test Split (Avoiding Leakage)"
        code: |
          import pandas as pd
          import numpy as np
          from sklearn.model_selection import train_test_split
          from sklearn.preprocessing import LabelEncoder

          # Create data
          np.random.seed(42)
          df = pd.DataFrame({
              'category': np.random.choice(['A', 'B', 'C', 'D'], 100),
              'value': np.random.randn(100),
              'target': np.random.choice([0, 1], 100)
          })

          # Split first
          X = df[['category', 'value']]
          y = df['target']
          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

          print("✅ CORRECT: Fit on train, transform both")

          # Fit encoder on training data only
          le = LabelEncoder()
          le.fit(X_train['category'])

          # Transform both
          X_train['category_encoded'] = le.transform(X_train['category'])
          X_test['category_encoded'] = le.transform(X_test['category'])

          print(f"Training categories: {set(X_train['category'])}")
          print(f"Test categories: {set(X_test['category'])}")
          print(f"Encoder knows: {set(le.classes_)}")

          print("\n⚠️  WATCH OUT: Test set may have unseen categories!")
          print("Handle with: default value or 'handle_unknown' parameter")
        output: |
          ✅ CORRECT: Fit on train, transform both
          Training categories: {'A', 'B', 'C', 'D'}
          Test categories: {'A', 'B', 'C'}
          Encoder knows: {'A', 'B', 'C', 'D'}

          ⚠️  WATCH OUT: Test set may have unseen categories!
          Handle with: default value or 'handle_unknown' parameter

  exercise:
    title: "Encode Multiple Categorical Variables"
    instruction: |
      You have a dataset with different types of categorical variables. Apply appropriate encoding.

      Create a dictionary called `result` with:
      - 'color_label_A': encoded value for 'Red' using label encoding (int)
      - 'size_ordinal_M': encoded value for 'Medium' using ordinal encoding (float)
      - 'n_onehot_columns': number of columns after one-hot encoding 'category' (int)
      - 'city_frequency_NYC': frequency encoding value for 'NYC' (int)

    setup_code: |
      import pandas as pd
      import numpy as np
      from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

      df = pd.DataFrame({
          'color': ['Red', 'Blue', 'Green', 'Red', 'Blue', 'Green', 'Red', 'Blue'],
          'size': ['Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Medium', 'Small'],
          'category': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B'],
          'city': ['NYC', 'LA', 'NYC', 'NYC', 'Chicago', 'LA', 'NYC', 'Chicago']
      })

    starter_code: |
      # Apply different encoding strategies
      # TODO: Label encode 'color'
      # TODO: Ordinal encode 'size' (Small=0, Medium=1, Large=2)
      # TODO: One-hot encode 'category'
      # TODO: Frequency encode 'city'

      result = {
          'color_label_A': None,  # TODO: encoded value for 'Red'
          'size_ordinal_M': None,  # TODO: encoded value for 'Medium'
          'n_onehot_columns': None,  # TODO: number of one-hot columns for 'category'
          'city_frequency_NYC': None  # TODO: frequency of 'NYC'
      }

    solution: |
      # 1. Label encoding for color
      le_color = LabelEncoder()
      df['color_encoded'] = le_color.fit_transform(df['color'])
      # Get encoded value for 'Red'
      color_label_A = int(le_color.transform(['Red'])[0])

      # 2. Ordinal encoding for size
      size_order = ['Small', 'Medium', 'Large']
      ordinal_encoder = OrdinalEncoder(categories=[size_order])
      df['size_encoded'] = ordinal_encoder.fit_transform(df[['size']])
      # Get encoded value for 'Medium'
      size_ordinal_M = float(ordinal_encoder.transform([['Medium']])[0][0])

      # 3. One-hot encoding for category
      category_onehot = pd.get_dummies(df['category'], prefix='category')
      n_onehot_columns = category_onehot.shape[1]

      # 4. Frequency encoding for city
      frequency_map = df['city'].value_counts().to_dict()
      city_frequency_NYC = frequency_map['NYC']

      result = {
          'color_label_A': color_label_A,
          'size_ordinal_M': size_ordinal_M,
          'n_onehot_columns': n_onehot_columns,
          'city_frequency_NYC': city_frequency_NYC
      }

    validation:
      type: "value_check"
      checks:
        - variable: "result"
          type: "dict"
          keys:
            - "color_label_A"
            - "size_ordinal_M"
            - "n_onehot_columns"
            - "city_frequency_NYC"

    hints:
      - level: 1
        text: "Use LabelEncoder for color, OrdinalEncoder with ordered list for size, pd.get_dummies() for category, and value_counts() for city frequency."

      - level: 2
        text: |
          Steps:
          1. LabelEncoder: fit_transform on 'color', then transform(['Red'])
          2. OrdinalEncoder: categories=[['Small','Medium','Large']], transform([['Medium']])
          3. One-hot: pd.get_dummies(df['category']), count columns
          4. Frequency: df['city'].value_counts()['NYC']

      - level: 3
        code: |
          # Label encoding
          le = LabelEncoder()
          le.fit_transform(df['color'])
          color_label_A = int(le.transform(['Red'])[0])

          # Ordinal encoding
          oe = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])
          oe.fit_transform(df[['size']])
          size_ordinal_M = float(oe.transform([['Medium']])[0][0])

          # One-hot encoding
          n_onehot_columns = pd.get_dummies(df['category']).shape[1]

          # Frequency encoding
          city_frequency_NYC = df['city'].value_counts()['NYC']

  follow_up:
    challenges:
      - "Implement target encoding with proper cross-validation to avoid overfitting"
      - "Compare model performance with different encoding strategies"
      - "Build a custom encoder for handling unseen categories in test data"
      - "Create an automated encoding pipeline that selects optimal strategy"

    resources:
      - title: "Categorical Encoding Methods"
        url: "https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02"
      - title: "Sklearn Preprocessing"
        url: "https://scikit-learn.org/stable/modules/preprocessing.html"
      - title: "Target Encoding Best Practices"
        url: "https://maxhalford.github.io/blog/target-encoding/"

    next_lesson: "preprocessing_04"
